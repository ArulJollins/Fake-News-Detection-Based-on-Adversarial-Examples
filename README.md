# Fake-News-Detection-Based-on-Adversarial-Examples
Fake news detection systems based on deep learning are highly susceptible to adversarial attacks, where minor changes in the input text can mislead the model into incorrect predictions. To address this, a defense mechanism is proposed that monitors the internal behavior of the model rather than relying solely on external content features. We use a Bidirectional LSTM to extract semantic representations of news articles and pass them through a Dense layer with ReLU activation to capture neuron activation patterns. By modeling these patterns using the Poisson distribution and comparing them with new inputs via cosine similarity, adversarial deviations are effectively identified. This approach offers a key advancement by enabling adversarial detection without modifying the original model, making it lightweight, non-intrusive, and easily adaptable to existing fake news detection systems
